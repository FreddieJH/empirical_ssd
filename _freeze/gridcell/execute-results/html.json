{
  "hash": "26102699f319fd34ca85042286c1e023",
  "result": {
    "markdown": "---\ntitle: \"gridcell\"\neditor: visual\nbibliography: references.bib\ndate: \"23 March, 2023\"\n\nexecute:\n  freeze: auto  # re-render only when source changes\n---\n\n\n## Analysis set-up\n\n\n\n\n\n### Required R packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse, quietly = TRUE)\nlibrary(lubridate, quietly = TRUE)\nlibrary(rstan, quietly = TRUE)\n```\n:::\n\n\n### Loading data\n\nWe will use a subset of RLS data:\n\n1.  Method 1 (fish abundance, species ID, and body size class)\n2.  Australia only\n\nWe will also use only species that we know have minimal fishing pressure, these species were selected from the supplementary material of @audzijonyte2020.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observational-level data (body size, abundance, species, survey ID)\nobs_data <- \n  \"data/raw/obs_data_m1_aus.parquet\" |> \n  read_parquet()\n\n\n# Survey-level information\nsurvey_list <- \n  \"data/cleaned/survey_list_m1_aus.parquet\" |> \n  read_parquet() \n\n# Only non-target species\nnontarget_spp <- \n  \"output/data/selected_spp.rds\" |> \n  read_rds()\n\n\n# Reef Life Survey (RLS) size class bins\ncutoff <- c(\n  2.5, 5, 7.5, 10, 12.5, 15, 20, 25, 30, 35, 40, 50, \n  seq(from = 62.5, to = 200, by = 12.5), \n  seq(from = 250, to = 500, by = 50))\n\n\n# Body size bin upper and lower bounds\nrls_bins <- \n  tibble(\n    size_index = 0:length(cutoff),\n    size_class = c(0, cutoff),\n    bin_lwr = (size_class + lag(size_class))/2,\n    bin_upr = (size_class + lead(size_class))/2\n  ) |>\n  filter(size_index != 0)\n```\n:::\n\n\n### Wrangling data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gridding latitude and longitude, to be used later\nobs_data_full <- \n  obs_data |> \n  filter(species_name %in% nontarget_spp) |> \n  left_join(survey_list, by = \"survey_id\") |> \n  mutate(lat_grid = floor(latitude), \n         lon_grid = floor(longitude), \n         survey_year = year(survey_date)) |> \n  count(species_name, size_class, lat_grid, survey_year)\n```\n:::\n\n\n## Bayesian modelling\n\nWe want to start with a single species for simplicity. We want to fit a lognormal distribution for each year and for each location. In this case we define a location as the latitudinal grid-cell, for now this will be the proxy for temperature value.\n\n### Selecting a single species\n\n`Pempheris multiradiata` was selected as it had the most location-year combinations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabundant_spp <- \n  obs_data_full |> \n  count(species_name, lat_grid, survey_year) |> \n  arrange(desc(n)) |> \n  count(species_name) |> \n  arrange(desc(n)) |> \n  head(1) |> \n  pull(species_name)\n\n\nobs_data_singlespp <- \n  obs_data_full |> \n  filter(species_name == abundant_spp)  |> \n  add_count(species_name, \n            lat_grid, \n            survey_year, \n            name = \"n_sizeclasses\") |> \n  filter(n_sizeclasses >= 3) \n```\n:::\n\n\n### Visualising the body size distributions for this species\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_name <- \"p_singlespp_byyear_bygrid\" |> fig_output_path()\n\nif(!file.exists(plot_name)){\n  \n  obs_data_singlespp |> \n    ggplot() +\n    aes(\n      x = size_class, \n      y = n) +\n    geom_path() +\n    facet_grid(survey_year ~ lat_grid, \n               scales = \"free_y\") + \n  ggsave(filename = plot_name)\n}\n\ninclude_graphics(plot_name)\n```\n\n::: {.cell-output-display}\n![](output/figs/p_singlespp_byyear_bygrid.png){width=1050}\n:::\n:::\n\n\nThe question is, do the parameters of the lognormal distribution change from year to year and from location to location?\n\n### Normalising the predictor variables\n\nWe will Z-transform `latiude` and `survey_year`. To do that we take the difference of the observation and the overall mean (the residual) and divide that by the standard deviation of that variable.\n\n$$\nZ(x_i) = \\frac{\n(x_i-\\bar{x})\n}{\\sigma(x)}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlats <-   \n  obs_data_singlespp |> \n  uncount(n) |> \n  pull(lat_grid) \n\nyears <- \n    obs_data_singlespp |> \n  uncount(n) |> \n  pull(survey_year) \n\nmean_latgrid <- mean(lats)\nsd_latgrid <- sd(lats)\n\nmean_year <- mean(years)\nsd_year <- sd(years)\n\n# Z-transform the location and year\nstan_data <- \n  obs_data_singlespp |> \n  select(-n_sizeclasses) |> \n  uncount(n) |> \n  left_join(rls_bins, by = join_by(size_class)) |> \n  mutate(lat_grid_Z = (lat_grid-mean_latgrid)/sd_latgrid,\n         survey_year_Z = (survey_year-mean_year)/sd_year) |> \n  count(species_name, size_index, size_class, lat_grid_Z, survey_year_Z)\n```\n:::\n\n\n#### Model fitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data for stan model\ndata_list <- list(\n  n_sizebins = max(stan_data$size_index), \n  n_observations = nrow(stan_data), \n  sizebin_id = stan_data$size_index, \n  n_individuals = stan_data$n,\n  upper_sizebin = cutoff[1:max(stan_data$size_index)],\n  lat = stan_data$lat_grid_Z,\n  yr = stan_data$survey_year_Z\n)\n\n\n# model fitting\nif(!file.exists(\"output/stan/stan_lnorm_gridcells_fitted.rds\")){\n  stan(file = 'data/stan_models/lnorm_gridcells.stan', \n              data = data_list,\n              iter = 600, \n              warmup = 200, \n              chains = 3, \n              refresh = 200, \n              seed = 1) |> \n    write_rds(\"output/stan/stan_lnorm_gridcells_fitted.rds\")\n} \n\nmodel <- read_rds(\"output/stan/stan_lnorm_gridcells_fitted.rds\")\n```\n:::\n\n\n#### Parameter fit visualisation\n\nWe want to check that the chains have converged and the parameter estimates are sensible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_name <- \"p_stan_output_onespp\" |> fig_output_path()\n\nif(!file.exists(plot_name)){\n  \n    rstan::traceplot(object = model, \n                 # pars = model_par, \n                 inc_warmup = TRUE, \n                 ncol = 3) + \n  ggsave(filename = plot_name)\n  \n}\n\ninclude_graphics(plot_name)\n```\n\n::: {.cell-output-display}\n![](output/figs/p_stan_output_onespp.png){width=3300}\n:::\n:::\n\n\n#### Predictions\n\n##### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the mean value of a parameter from the stan model\nparam_meanval <- function(stan_model, param){\n   \n  summary(stan_model)$summary |> \n    as_tibble(rownames = \"parameter\") |> \n    filter(parameter == param) |> \n    pull(mean)\n  \n}\n\n# Extract the cumulative probability of being less than a given size class, and for a specific latitude and year\nmodel_predict <- function(stan_model, size_class, lat_Z, year_Z){\n\n  meanlog_est <- \n    param_meanval(stan_model, \"ln_mu\") +\n    (param_meanval(stan_model, \"ln_mu_lat\")*lat_Z) +\n    (param_meanval(stan_model, \"ln_mu_year\")*year_Z) \n  \n  sdlog_est <- \n    param_meanval(stan_model, \"ln_sigma\") +\n    (param_meanval(stan_model, \"ln_sigma_lat\")*lat_Z) +\n    (param_meanval(stan_model, \"ln_sigma_year\")*year_Z) \n  \n  plnorm(q = size_class, \n         meanlog = meanlog_est, \n         sdlog = exp(sdlog_est))\n  \n}\n```\n:::\n\n\n##### Predicting probability\n\nTo calculate the probability of being in a certain bin, we want to calculate the cumulative probability of being less than a given size bin (e.g. \\`p(x\\>6.75cm)\\` based on the estimated lognormal distribution for that year\\*location combination. The probability of being in that bin will be the probability of being less than the upper limit of that bin minus the probability of being less than the lower limit of that bin.\n\nFor estimating we will create a 'prediction table', which is all the combinations of RLS body size bins, latitudes and years (for this species), and then use the fitted stan model to make predictions based on those estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_table <-\n  tibble(\n    size_class = rls_bins$size_class) |> \n  expand_grid(lat_grid = sort(unique(obs_data_singlespp$lat_grid))) |> \n  expand_grid(survey_year = sort(unique(obs_data_singlespp$survey_year))) |> \n  left_join(rls_bins, by = join_by(size_class)) |> \n  mutate(lat_z = (lat_grid-mean_latgrid)/sd_latgrid,\n         year_z = (survey_year-mean_year)/sd_year) |> \n  mutate(upr_pred = model_predict(model, bin_upr, lat_z, year_z),\n         lwr_pred = model_predict(model, bin_lwr, lat_z, year_z)) |> \n  mutate(prob = upr_pred-lwr_pred)\n\n\nprediction_table |> \n  arrange(lat_grid, survey_year, size_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14,880 x 11\n   size_c~1 lat_g~2 surve~3 size_~4 bin_lwr bin_upr lat_z year_z upr_p~5 lwr_p~6\n      <dbl>   <dbl>   <dbl>   <int>   <dbl>   <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n 1      2.5     -44    1992       1    1.25    3.75 -1.26  -2.21   0.800   0.415\n 2      5       -44    1992       2    3.75    6.25 -1.26  -2.21   0.909   0.800\n 3      7.5     -44    1992       3    6.25    8.75 -1.26  -2.21   0.951   0.909\n 4     10       -44    1992       4    8.75   11.2  -1.26  -2.21   0.971   0.951\n 5     12.5     -44    1992       5   11.2    13.8  -1.26  -2.21   0.982   0.971\n 6     15       -44    1992       6   13.8    17.5  -1.26  -2.21   0.990   0.982\n 7     20       -44    1992       7   17.5    22.5  -1.26  -2.21   0.995   0.990\n 8     25       -44    1992       8   22.5    27.5  -1.26  -2.21   0.997   0.995\n 9     30       -44    1992       9   27.5    32.5  -1.26  -2.21   0.998   0.997\n10     35       -44    1992      10   32.5    37.5  -1.26  -2.21   0.999   0.998\n# ... with 14,870 more rows, 1 more variable: prob <dbl>, and abbreviated\n#   variable names 1: size_class, 2: lat_grid, 3: survey_year, 4: size_index,\n#   5: upr_pred, 6: lwr_pred\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_name <- \"p_singlespp_byyear_bygrid_pred_p\" |> fig_output_path()\n\nif(!file.exists(plot_name)){\n  \n  prediction_table |> \n    ggplot() +\n    aes(x = size_class, \n        y = prob) +\n    geom_line() +\n    facet_grid(survey_year ~ lat_grid, scales = \"free_y\") +\n    lims(\n      x = c(0, 50)\n    ) + \n    ggsave(filename = plot_name, \n           width = 22, \n           height = 12.5)\n  \n}\n\ninclude_graphics(plot_name)\n```\n\n::: {.cell-output-display}\n![](output/figs/p_singlespp_byyear_bygrid_pred_p.png){width=3300}\n:::\n:::\n\n\n##### Comparing observed and estimated\n\nTo compare the model predictions with the observed values we need to calculate the observed probability of being in each RLS size bin. This is calculated as the abundance in that bin divided by the total abundance in that group (i.e. year-by-location-by-species combination).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_data_singlespp_prob <- \n  obs_data_singlespp |> \n  add_count(lat_grid, survey_year, \n            wt = n,\n            name = \"total_n\") |> \n  mutate(prob_obs = n/total_n) |> \n  select(size_class, lat_grid, survey_year, prob_obs) |> \n  distinct()\n```\n:::\n\n\nPlotting all the sites on a single figure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_name <- \"p_singlespp_byyear_bygrid_pred_prob_all\" |> fig_output_path()\n\nif(!file.exists(plot_name)){\n  \n  prediction_table |> \n  left_join(obs_data_singlespp_prob) |> \n  mutate(prob_obs = replace_na(prob_obs, 0)) |> \n  pivot_longer(cols = c(prob, prob_obs), \n               names_to = \"prob_type\", \n               values_to = \"probability\") |> \n  ggplot() +\n  aes(x = size_class,\n      y =probability, \n      col = prob_type\n        ) +\n  geom_point() +\n  geom_line() + \n  facet_grid(survey_year ~ lat_grid, scales = \"free_y\") +\n  xlim(c(0, 50)) + \n    ggsave(filename = plot_name, \n           width = 22, \n           height = 12.5)\n  \n}\n\ninclude_graphics(plot_name)\n```\n\n::: {.cell-output-display}\n![](output/figs/p_singlespp_byyear_bygrid_pred_prob_all.png){width=3300}\n:::\n:::\n\n\n...or a subset of sites (for easier visualisation)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_name <- \"p_singlespp_byyear_bygrid_pred_prob_subset\" |> fig_output_path()\n\nif(!file.exists(plot_name)){\n  \n  prediction_table |> \n  left_join(obs_data_singlespp_prob) |> \n  mutate(prob_obs = replace_na(prob_obs, 0)) |> \n  pivot_longer(cols = c(prob, prob_obs), \n               names_to = \"prob_type\", \n               values_to = \"probability\") |> \n    filter(survey_year > 2010, \n         lat_grid > -35) |> \n  ggplot() +\n  aes(x = size_class,\n      y =probability, \n      col = prob_type\n        ) +\n  geom_point() +\n  geom_line() + \n  facet_grid(survey_year ~ lat_grid, scales = \"free_y\") +\n  xlim(c(0, 50)) + \n    ggsave(filename = plot_name, \n           width = 22, \n           height = 12.5)\n  \n}\n\ninclude_graphics(plot_name)\n```\n\n::: {.cell-output-display}\n![](output/figs/p_singlespp_byyear_bygrid_pred_prob_subset.png){width=3300}\n:::\n:::\n\n\n## Multiple distribution fitting\n\nThe model doesn't appear to fit that well. It would be good to fit a lognormal distribution to each year-by-location for the species, then we can look at the variability in these parameters for a single species.\n\nIs a single distribution for a single species even a good approximation?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncombinations_table <- \n obs_data_singlespp |> \n  select(lat_grid, \n         survey_year) |> \n  distinct() |> \n  mutate(combination_id = row_number())\n\n\n# Z-transform the location and year\nstan_data_combinations <- \n  obs_data_singlespp |> \n  select(-n_sizeclasses) |> \n  uncount(n) |> \n  left_join(rls_bins, \n            by = join_by(size_class)) |> \n  left_join(combinations_table, \n            by = join_by(lat_grid, survey_year)) |> \n  mutate(lat_grid_Z = (lat_grid-mean_latgrid)/sd_latgrid,\n         survey_year_Z = (survey_year-mean_year)/sd_year) |> \n  count(species_name, \n        size_index, \n        size_class, \n        combination_id, \n        lat_grid_Z, \n        survey_year_Z)\n\n\n# Data for multiple distribution stan model\ndata_list_combinations <- list(\n  n_combinations = max(stan_data_combinations$combination_id),\n  n_sizebins = max(stan_data_combinations$size_index), \n  n_observations = nrow(stan_data_combinations), \n  sizebin_id = stan_data_combinations$size_index, \n  combination_id = stan_data_combinations$combination_id,\n  n_individuals = stan_data_combinations$n,\n  upper_sizebin = cutoff[1:max(stan_data_combinations$size_index)]\n)\n\nfile_name <- \"stan_lnorm_gridcells_combinations_fitted\" |> stan_output_path()\nstan_file <- \"lnorm_gridcells_combinations\" |> stan_input_path()\n\n# model fitting\nif(!file.exists(file_name)){\n  stan(file = stan_file, \n              data = data_list_combinations,\n              iter = 600, \n              warmup = 200, \n              chains = 3, \n              refresh = 200, \n              seed = 1) |> \n    write_rds(file_name)\n} \n\nmodel_combinations <- \n  read_rds(file_name)\n```\n:::\n\n\nLet's look at the distribution of the lognormal parameters for each combination of year and location.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_par <- function(stan_model, parameter){\n  stan_model |> \n  extract() |> \n  pluck(parameter) |> \n  as_tibble(rownames = \"iteration\") |> \n  pivot_longer(cols = -iteration, \n               names_prefix = \"V\",\n               names_transform = list(combination_id = as.numeric),\n               names_to = \"combination_id\") |> \n  summarise(!!parameter := mean(value), .by = combination_id)\n}\n\n\nmodel_combinations_pars <-\n  get_par(model_combinations, \"ln_sigma\") |> \n  left_join(get_par(model_combinations, \"ln_mu\"), \n            by = join_by(combination_id)) |> \n  left_join(combinations_table, \n            by = join_by(combination_id))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_combinations_pars |> \n  ggplot() +\n  aes(\n    x = ln_mu\n  ) +\n  geom_density()\n```\n\n::: {.cell-output-display}\n![](gridcell_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel_combinations_pars |> \n  ggplot() +\n  aes(\n    x = ln_sigma |> exp()\n  ) +\n  geom_density()\n```\n\n::: {.cell-output-display}\n![](gridcell_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel_combinations_pars |> \n  ggplot() +\n  aes(\n    y = ln_sigma |> exp(),\n    x = ln_mu \n  ) +\n  geom_point() +\n  labs(x = \"meanlog\", y = \"sdlog\")\n```\n\n::: {.cell-output-display}\n![](gridcell_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel_combinations_pars |> \n  ggplot() +\n  aes(\n    x = as.factor(lat_grid), \n    y = ln_mu\n  ) +\n  geom_violin()\n```\n\n::: {.cell-output-display}\n![](gridcell_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel_combinations_pars |> \n  ggplot() +\n  aes(\n    x = as.factor(lat_grid), \n    y = ln_sigma |> exp()\n  ) +\n  geom_violin()\n```\n\n::: {.cell-output-display}\n![](gridcell_files/figure-html/unnamed-chunk-3-5.png){width=672}\n:::\n:::\n\n\n## Multiple species\n\nNow we want to run the Stan code on many species, not just one.\n\nI decided to create all the observed combinations of species, lat, and year, rather than all of the possible combinations (a matrix of unique species by year by location) as that would create many empty combinations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_data_allspp <- \n  obs_data_full |> \n  add_count(species_name, \n            lat_grid, \n            survey_year, \n            name = \"n_sizeclasses\") |> \n  filter(n_sizeclasses >= 3) |> \n  add_count(species_name, \n            lat_grid, \n            survey_year, \n            wt = n,\n            name = \"total_n\") |> \n  mutate(prob_obs = n/total_n) \n\n\ncombinations_table_allspp <- \n obs_data_allspp |> \n  select(species_name, \n         lat_grid, \n         survey_year) |> \n  distinct() |> \n  mutate(combination_id = row_number())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Z-transform the location and year\nstan_data_combinations_allspp <- \n  obs_data_allspp |> \n  select(-n_sizeclasses) |> \n  uncount(n) |> \n  left_join(rls_bins, \n            by = join_by(size_class)) |> \n  left_join(combinations_table_allspp, \n            by = join_by(species_name, lat_grid, survey_year)) |> \n  mutate(lat_grid_Z = (lat_grid-mean_latgrid)/sd_latgrid,\n         survey_year_Z = (survey_year-mean_year)/sd_year) |> \n  count(species_name, \n        size_index, \n        size_class, \n        combination_id, \n        lat_grid_Z, \n        survey_year_Z)\n\n# Data for multiple distribution stan model\ndata_list_combinations_allspp <- list(\n  n_combinations = max(stan_data_combinations_allspp$combination_id),\n  n_sizebins = max(stan_data_combinations_allspp$size_index), \n  n_observations = nrow(stan_data_combinations_allspp), \n  sizebin_id = stan_data_combinations_allspp$size_index, \n  combination_id = stan_data_combinations_allspp$combination_id,\n  n_individuals = stan_data_combinations_allspp$n,\n  upper_sizebin = cutoff[1:max(stan_data_combinations_allspp$size_index)]\n)\n\nfile_name <- \n  \"stan_lnorm_gridcells_combinations_fitted_allspp\" |> \n  stan_output_path()\nstan_file <- \"lnorm_gridcells_combinations\" |> stan_input_path()\n\n# model fitting\nif(!file.exists(file_name)){\n  stan(file = stan_file, \n              data = data_list_combinations_allspp,\n              iter = 600, \n              warmup = 200, \n              chains = 3, \n              refresh = 200, \n              seed = 1) |> \n    write_rds(file_name)\n} \n\nmodel_combinations_allspp <- \n  read_rds(file_name)\n```\n:::\n\n\nmean of the lognormal distribution is:\n\n$$\nm=e^{\\mu + 2\\sigma^2}â€‹,\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \n# model_combinations_allspp_pars <-\n#   get_par(model_combinations, \"ln_sigma\") |> \n#   left_join(get_par(model_combinations_allspp, \"ln_mu\"), \n#             by = join_by(combination_id)) |> \n#   left_join(combinations_table_allspp, \n#             by = join_by(combination_id)) |> \n#   mutate(mean_size = exp(ln_mu + (2*(ln_sigma^2))))\n# \n# \n# model_combinations_allspp_pars |> \n#   ggplot() +\n#   aes(\n#     x = lat_grid, \n#     y = ln_mu, \n#     colour = species_name\n#   ) +\n#   geom_path() +\n#   theme(legend.position = \"none\")\n# \n# \n# \n# lm(ln_mu ~ lat_grid + species_name + survey_year, data = model_combinations_allspp_pars)\n# \n# \n# mod1 <- lmerTest::lmer(mean_size ~ lat_grid + (1|species_name) + survey_year, data = model_combinations_allspp_pars)\n# summary(mod1)\n# \n# \n# # higher latitudes (warmer) have smaller species\n# \n# \n# \n# \n# predict(object = mod1, newdata = list())\n# \n# model_combinations_allspp_pars$mean_size |> hist()\n# \n# \n# \n# model_combinations_allspp_pars |> \n#   ggplot() +\n#   aes(x = ln_mu, \n#       colour = species_name) +\n#   geom_density()+\n#   theme(legend.position = \"none\")\n# \n# model_combinations_allspp_pars |> \n#   ggplot() +\n#   aes(exp(ln_sigma)) +\n#   geom_density()\n# \n# \n# \n# model_combinations_allspp_pars |> \n#   mutate(mean_size = exp(ln_mu + (2*(ln_sigma^2))))\n# \n# new_data <- \n# obs_data_allspp |> \n#   uncount(weights = n) |> \n#   group_by(species_name, lat_grid, survey_year) |> \n#   summarise(mean_size = mean(size_class))\n# \n# \n# mod2 <- lmerTest::lmer(mean_size ~ lat_grid*survey_year + (1|species_name), data = new_data)\n# summary(mod2)\n# \n# \n# \n# for(i in unique(new_data$species_name)){\n#   file_name  <- paste0(\"output/figs/mean_size_trends/\", i, \".png\")\n#   if(!file.exists(file_name)){\n#       p <-\n#     new_data |> \n#   filter(species_name == i) |> \n#   ggplot(aes(x = survey_year,\n#              y = mean_size)) +\n#   geom_path() +\n#   facet_wrap(~lat_grid) \n#   \n#   \n#   ggsave(filename = file_name, \n#          plot = p)\n#   }\n#   \n# \n# }\n```\n:::\n",
    "supporting": [
      "gridcell_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}